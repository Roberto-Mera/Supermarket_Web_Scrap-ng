{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98de57c3",
      "metadata": {
        "id": "98de57c3",
        "outputId": "986b1676-bf6b-446c-e458-1222ee3c5466",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: undetected_chromedriver in c:\\programdata\\anaconda3\\lib\\site-packages (3.5.5)Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "Requirement already satisfied: selenium>=4.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from undetected_chromedriver) (4.27.1)\n",
            "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from undetected_chromedriver) (2.32.2)\n",
            "Requirement already satisfied: websockets in c:\\programdata\\anaconda3\\lib\\site-packages (from undetected_chromedriver) (14.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected_chromedriver) (2.2.2)\n",
            "Requirement already satisfied: trio~=0.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium>=4.9.0->undetected_chromedriver) (0.27.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium>=4.9.0->undetected_chromedriver) (0.11.1)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium>=4.9.0->undetected_chromedriver) (2024.8.30)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium>=4.9.0->undetected_chromedriver) (4.11.0)\n",
            "Requirement already satisfied: websocket-client~=1.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium>=4.9.0->undetected_chromedriver) (1.8.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->undetected_chromedriver) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->undetected_chromedriver) (3.7)\n",
            "Requirement already satisfied: attrs>=23.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected_chromedriver) (24.2.0)\n",
            "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected_chromedriver) (2.4.0)\n",
            "Requirement already satisfied: outcome in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected_chromedriver) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected_chromedriver) (1.3.0)\n",
            "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected_chromedriver) (1.16.0)\n",
            "Requirement already satisfied: wsproto>=0.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium>=4.9.0->undetected_chromedriver) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected_chromedriver) (1.7.1)\n",
            "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium>=4.9.0->undetected_chromedriver) (2.21)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium>=4.9.0->undetected_chromedriver) (0.14.0)\n"
          ]
        }
      ],
      "source": [
        "pip install undetected_chromedriver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "440abac2",
      "metadata": {
        "id": "440abac2"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import random\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from selenium import webdriver\n",
        "import undetected_chromedriver as uc\n",
        "import gc\n",
        "import csv\n",
        "\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.common.by import By\n",
        "import re\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "124cb526",
      "metadata": {
        "id": "124cb526"
      },
      "source": [
        "# Ripley Web Scraping\n",
        "\n",
        "En esta sección conseguiremos mediante web scraping de la página de Ripley. Nos centraremos en las categorías de vestimenta y calzado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2caed6a",
      "metadata": {
        "id": "a2caed6a"
      },
      "outputs": [],
      "source": [
        "categorias=[\"hombre\", \"mujer\", \"moda-infantil\",\"calzado\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a394cdb0",
      "metadata": {
        "id": "a394cdb0"
      },
      "outputs": [],
      "source": [
        "# Funciones auxiliares\n",
        "def acceso(url, browser):\n",
        "    \"\"\"\n",
        "    Función para acceder a la página web y extraer el HTML\n",
        "    \"\"\"\n",
        "    browser.get(url)\n",
        "    browser.implicitly_wait(5)\n",
        "\n",
        "    # Cierra las ventanas de suscripción y ubicación\n",
        "    cerrar_ventanas(browser)\n",
        "\n",
        "    html = browser.page_source\n",
        "    soup = bs(html, \"html.parser\")\n",
        "    return soup\n",
        "\n",
        "def cerrar_ventanas(browser):\n",
        "    \"\"\"\n",
        "    Función para cerrar las ventanas emergentes (suscripción, ubicación, etc.)\n",
        "    \"\"\"\n",
        "    xpaths = [\n",
        "        '//*[@class=\"align-right secondary slidedown-button\"]',  # Ventana de suscripción\n",
        "        '//*[@class=\"tooltip-button-close\"]'  # Ventana de ubicación\n",
        "    ]\n",
        "\n",
        "    for xpath in xpaths:\n",
        "        try:\n",
        "            browser.find_element(\"xpath\", xpath).click()\n",
        "        except:\n",
        "            pass  # Si no se encuentra el elemento, continúa con el siguiente xpath\n",
        "\n",
        "def number_page(enlace, browser):\n",
        "    \"\"\"\n",
        "    Función para obtener la cantidad de páginas de una categoría\n",
        "    \"\"\"\n",
        "    soup = acceso(enlace, browser)\n",
        "    articulos = int(soup.find(\"div\", {\"class\": \"col-xs-12 col-md-9 col-lg-9\"}).text.split(\" \")[0])\n",
        "    return articulos // 48 + 1\n",
        "\n",
        "def next_page(enlace, pagina):\n",
        "    \"\"\"\n",
        "    Función para generar la URL de la página i\n",
        "    \"\"\"\n",
        "    return f\"{enlace}?page={pagina}\"\n",
        "\n",
        "def text_extract(textos):\n",
        "    \"\"\"\n",
        "    Función para extraer el texto de los elementos encontrados\n",
        "    \"\"\"\n",
        "    return [texto.text for texto in textos]\n",
        "\n",
        "def prices_extract(precios):\n",
        "    \"\"\"\n",
        "    Función para extraer el precio de los elementos encontrados\n",
        "    \"\"\"\n",
        "    return [precio.text.strip()[3:] for precio in precios]\n",
        "\n",
        "def limpiar_memoria():\n",
        "    \"\"\"\n",
        "    Función para liberar memoria eliminando objetos innecesarios\n",
        "    \"\"\"\n",
        "    gc.collect()\n",
        "\n",
        "def obtener_datos_categoria(url, browser):\n",
        "    \"\"\"\n",
        "    Generador para obtener datos de productos por categoría.\n",
        "    Este generador ayudará a manejar grandes cantidades de datos de manera eficiente.\n",
        "    \"\"\"\n",
        "    n = number_page(url, browser) + 1\n",
        "    for i in range(1, n):\n",
        "        url_page = next_page(url, i)\n",
        "        time.sleep(random.uniform(2, 5))  # Pausa aleatoria entre 2 y 5 segundos\n",
        "        soup = acceso(url_page, browser)\n",
        "\n",
        "        items = soup.find_all(\"div\", {\"class\": \"catalog-product-details__name\"})\n",
        "        marcas = soup.find_all(\"div\", {\"class\": \"brand-logo\"})\n",
        "        precios = soup.find_all(\"li\", {\"title\": \"Precio Internet\"})\n",
        "\n",
        "        items_list = text_extract(items)\n",
        "        marcas_list = text_extract(marcas)\n",
        "        precios_list = prices_extract(precios)\n",
        "\n",
        "        # Devuelve los resultados como un lote, y no los guarda en memoria\n",
        "        yield items_list, marcas_list, precios_list\n",
        "\n",
        "        # Liberar memoria después de procesar cada página\n",
        "        del soup\n",
        "        limpiar_memoria()\n",
        "\n",
        "browser = uc.Chrome()\n",
        "\n",
        "# Inicialización de listas globales para los datos de todas las categorías\n",
        "items_totales = []\n",
        "marcas_totales = []\n",
        "precios_totales = []\n",
        "categorias_totales = []\n",
        "\n",
        "# Lista de categorías a procesar\n",
        "categorias = [\"hombre\", \"mujer\", \"moda-infantil\", \"calzado\"]\n",
        "\n",
        "# Escribir los datos en un archivo CSV de manera incremental\n",
        "with open('Ripley.csv', mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    # Escribir encabezados en el archivo CSV\n",
        "    writer.writerow([\"Item\", \"Marca\", \"Precio\", \"Categoria\", \"Web\", \"Fecha\"])\n",
        "\n",
        "    # Recorrer cada categoría y obtener los datos\n",
        "    for categoria in categorias:\n",
        "        url = f\"https://simple.ripley.com.pe/{categoria}\"\n",
        "\n",
        "        # Usar el generador para obtener los datos de cada página\n",
        "        for items_list, marcas_list, precios_list in obtener_datos_categoria(url, browser):\n",
        "            for item, marca, precio in zip(items_list, marcas_list, precios_list):\n",
        "                # Escribir los datos de cada producto en el archivo CSV\n",
        "                writer.writerow([item, marca, precio, categoria, \"Ripley\", datetime.now().strftime(\"%m/%d/%Y\")])\n",
        "\n",
        "            # Liberar memoria\n",
        "            del items_list, marcas_list, precios_list\n",
        "            gc.collect()\n",
        "\n",
        "browser.quit()\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}